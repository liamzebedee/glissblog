const data = {
 "anatomy-of-a-stark-proof": {
  "slug": "anatomy-of-a-stark-proof",
  "content": "# What is the anatomy of a STARK proof?\n\nThis was a question I had while analysing how to implement recursive proofing, otherwise known as a STARK verifier inside a provable environment.\n\n**Thank you to**: [Louis Guthmann](https://twitter.com/GuthL) for helping connect me and answering my early questions around STARK's and SNARK's, as well [Kobi Gurkan](https://twitter.com/kobigurk) (who [explained R1CS to me](https://twitter.com/liamzebedee/status/1542383265708793857)), [Th0rgal_](https://twitter.com/Th0rgal_) for volunteering to tutor me on the polynomial maths, Alan Szepieniec for his fantastic [public guide on STARK's](https://aszepieniec.github.io/stark-anatomy/overview).\n\n## Background.\n\nTurns out, STARK's are really simple! Here are some topics and concepts that will come up during your explorations. \n\n * constraint systems, satisfiability\n     * Fun question - how do you convert a program into a set of equations?\n * arithmetic and polynomial equations\n * the idea of a \"finite field\", field elements\n     * Fun question - how come Cairo requires a separate operator for greater-than/less-than comparisons?\n * basic interpolation, bonus pts. if you understand FFT\n * verifier-prover computation model, interactive vs. non-interactive protocols, Fiat-Shamir transform\n * commitment schemes - operations of (commit, open/reveal, verify/prove), merkle trees\n * degree of a polynomial, [unicity theorem](https://math.stackexchange.com/questions/240871/understanding-proof-of-uniqueness-in-theorem-on-polynomial-interpolation)\n * [FRI protocol](https://aszepieniec.github.io/stark-anatomy/fri.html), which is basically a [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm).\n\n### Simple analogies for common concepts.\n\n * [Commitment scheme](https://en.wikipedia.org/wiki/Commitment_scheme) - basically a generalisation of merkle trees. The merkle root is in essence, a commitment.\n * FRI is a [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm) for the polynomial to find an area where it fails the low-degreeness test.\n * [Fiat-Shamir](https://en.wikipedia.org/wiki/Fiat%E2%80%93Shamir_heuristic) - hashes make people (the prover) commit to things (same idea as atomic swaps), which prevents them from aborting a protocol (like a swap).\n * [Reed-Solomon](https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction) - this tech is usually used for [error correction](https://en.wikipedia.org/wiki/Error_correction_code) - ie. redundantly encode data, so if a bit flips / the cable does something wack, you can still recover the full data. \n   If you look at how Reed-Solomun works, it's just based on interpolating that data into a polynomial curve, and sampling points from it (redundancy). FRI just uses Reed-Solomun to sample random points for the binary search\n   \n## ZK-STARK's - how are they generated?\n\nThe **best** explanation I've ever read of ZK-STARK's is from a cryptographer named [Alan Szepieniec](https://twitter.com/aszepieniec), who has written [an entire public series here](https://aszepieniec.github.io/stark-anatomy/overview). The diagrams and content below in **this section** are mostly lifted from his writing. \n\nThese are the basic steps to generating a STARK proof from running a provable program:\n\n![](https://i.imgur.com/arAByNL.png)\n\n 1. **Arithmetization**. The first transformation in the pipeline is known as arithmetization. In this procedure, the sequence of elementary logical and arithmetical operations on strings of bits is transformed into a sequence of native finite field operations on finite field elements, such that the two represent the same computation. The output is an arithmetic constraint system, essentially a bunch of equations with coefficients and variables taking values from the finite field.\n\n 3. **Interpolation**. Interpolation in the usual sense means finding a polynomial that passes through a set of data points. In the context of the STARK compilation pipeline, interpolation means finding a representation of the arithmetic constraint system in terms of polynomials. The interpolation step reduces the satisfiability of an arithmetic constraint system to a claim about the low degree of certain polynomials - ie. from $x=0$ to $deg (f)$. The resulting object is an abstract protocol called a Polynomial Interactive oracle proof (IOP). \n\n 4. **Proving low-degreeness**. The protocol designer who wants to use a Polynomial IOP as an intermediate stage must find a way to commit to a polynomial and then open that polynomial in a point of the verifier’s choosing. FRI is a key component of a STARK proof that achieves this task by using Merkle trees of Reed-Solomon Codewords to prove the boundedness of a polynomial’s degree. After applying the Fiat-Shamir transform to make it non-interactive, you get a probabilistic proof that at least some high percentage (eg. 80%) of a given set of values represent the evaluations of some specific polynomial whose degree is much lower than the number of values given. This is the STARK.\n\n### How can proving low-degreeness be related to proving the satisfiability of the polynomial?\n\nThis is a question I had, and didn't know the answer to! So [we investigated](https://twitter.com/liamzebedee/status/1591785700327919617).\n\n**Problem**. How is the problem of arithmetic satisfiability converted into proving low-degreeness of a polynomial?\n\n**Answer**. Using the unicity theorem of polynomials + blowing up the domain to make it intractable to find a \"colliding\" polynomial. See [[1]](https://math.stackexchange.com/questions/240871/understanding-proof-of-uniqueness-in-theorem-on-polynomial-interpolation), [[2]](https://en.wikipedia.org/wiki/Polynomial_interpolation)\n\n> **Polynomial unicity theorem.**\n> There exists a unique polynomial of degree at most $n$ that interpolates the $n+1$ data points $(x_0,y_0),\\dotsc,(x_n,y_n) \\in \\mathbb {R} ^2$, where no two $x_j$ are the same.\n\nBasically:\n\n - polynomial has roots, where $f(x)=0$\n- there is a theorem which relates the roots of a polynomial with the [degree](https://en.wikipedia.org/wiki/Degree_of_a_polynomial)\n- this is the unicity theorem\n- if you can construct the unique polynomial of degree at most $n$, for these roots, then proving the existence of this polynomial (impossible) is equivalent to proving the low-degreeness of the polynomial (possible using FRI)\n- problem though - what stops me constructing a polynomial of a similar degree less than $n$, but with completely different values to those in the trace? \n- approach is to \"blow up\" the polynomial's domain, such that finding a 2nd \"exploit\" polynomial as described would be intractable according to some security assumptions. \n- this is called the **low-degree extension**. Quoting Starkware:\n\n> N. In order to achieve a secure protocol, each such polynomial is evaluated over a larger domain, which we call the evaluation domain. For example, in our StarkDEX Alpha the size of the evaluation domain is usually 16*N. We refer to this evaluation as the trace Low Degree Extension (LDE) and the ratio between the size of the evaluation domain and the trace domain as the blowup factor (those familiar with coding theory notation will notice that the blowup factor is 1/rate and the LDE is in fact simply a Reed-Solomon code of the trace).\n\n\n<!-- \n\ny = 1x + 2x - 1\nRoots of your polynomial. There is only 1 root\nwhere y=0\nroot := x, where f(x)=y=0\nyou can rewrite as:\n0/0 = (1x + 2x - 1) / (x - y)\n\nYou can always rewrite in the form\n(x - A)(x - B) ... for the roots (A, B, ...)\n(x - 1)(x - 2) - graph this on geogebra\n\nSo we can rewrite this relation\nB(x) = 0, when x=y\ny is a root of B(x)\nthus we can rewrite as \n(x - y) divides B(x)\n\nhttps://math.stackexchange.com/questions/240871/understanding-proof-of-uniqueness-in-theorem-on-polynomial-interpolation\n\n -->\n\n\n### Concepts.\n\n - **Execution Trace**. A series of registers over time.\n - **Constraints** / **Trace Polynomials** / **AIR constraints**. \n     - expressed as polynomials composed over the trace cells that are satisfied if and only if the computation is correct. \n     - aka _Algebraic Intermediate Representation (AIR) Polynomial Constraints_.\n     - Examples: \n         - transition constraints - program counter \n         - boundary constraints\n - **Composition polynomial**.\n     - A combination of the constraints into a single (larger) polynomial, so that a single low degree test can be used to attest to their low degree\n     - Only used in Starkware's STARK's (see ethstark)\n\n\n## ZK-STARK's - how are they verified?\n\nTo verify the validity of the execution trace:\n\n 1. Read the trace and constraint commitments. (reveal their leaf in the merkle tree)\n 2. Draw pseudo-random query positions for the LDE domain from the public coin\n 3. Read evaluations of trace and constraint composition polynomials at the queried positions;\n 4. For each queried trace/constraint state, combine column values into a single value by computing their random linear combinations. \n 5. [Combine trace and constraint compositions together](https://github.com/novifinancial/winterfell/blob/main/verifier/src/composer.rs#L179:12).\n 6. Compute evaluations of the DEEP composition polynomial at the queried positions.\n 7. Verify low-degree proof. Make sure that evaluations of the DEEP composition polynomial we computed in the previous step are in fact evaluations of a polynomial of degree equal to trace polynomial degree. This uses the positions from step (2).\n\nBasically:\n\n * The computation is expressed as both the trace and the constraints. Of course we cannot know the validity of the constraints without the trace values. We use a public coin to seed our selection of positions, and then read the commitments, evaluate the polynomials at those committed positions, combine their values into the composition polynomial, and then do our degree-ness test using FRI.\n\nThere is good docs in the [Winterfell verifier code](https://github.com/novifinancial/winterfell/blob/main/verifier/src/lib.rs#L182). And here's a [summary of the Winterfell verification](https://gist.github.com/liamzebedee/a5864ec950ab13f566f0104fe1711681).\n\n\n## Data structure for a STARK proof.\n\nThis section is WIP. Basically I want to document what a STARK proof looks like at the data level.\n\n\n### Circom.\n\nIf you look at an implementation of a STARK verifier (e.g. [in Circom](https://www.notion.so/Roadmap-e116a4bdccf5441c98f60f85dccaa3fa)), you can see the data structure:\n\n- Trace\n    - Commitment - root of the trace merkle tree\n    - Evaluations - trace polynomial evaluations at the query positions\n    - Query proofs - authentication paths of the aforementionned merkle tree at the query positions\n- Constraint (transition/boundary) merkle tree.\n    - Root - termed the “constraint commitment”.\n    - Evaluations - polynomial evaluations.\n    - Query proofs - merkle authentication paths to check consistency between the commitment and the queries at pseudo-random position\n- FRI.\n    - Commitments - the root of the evaluations merkle tree for each FRI layer\n    - Proofs - authentication paths of the aforementionned merkle tree at the query_positions for each FRI layer\n    - Queries - DEEP\n- Misc:\n    - OOD - out-of-domain - basically **boundary constraints**.\n    - pub_coin_seed - serialized public inputs and context to initialize the public coin\n    - pow_nonce - nonce for the proof of work determined by the grinding factor in the proof options\n\n### Starkware.\n\nWIP.\n\n```\nstruct StarkWitness {\n\t    traces_decommitment: TracesDecommitment*,\n\t    traces_witness: TracesWitness*,\n\t    composition_decommitment: TableDecommitment*,\n\t    composition_witness: TableCommitmentWitness*,\n\t    fri_witness: FriWitness*,\n\t}\n```\n\n[https://sourcegraph.com/github.com/starkware-libs/cairo-lang@master/-/blob/src/starkware/cairo/stark_verifier/core/table_commitment.cairo](https://sourcegraph.com/github.com/starkware-libs/cairo-lang@master/-/blob/src/starkware/cairo/stark_verifier/core/table_commitment.cairo)\n\n## Misc.\n\nhttps://hackmd.io/@vbuterin/snarks#Can-we-have-one-more-recap-please"
 },
 "dumb-primer-on-recursive-zk-proofs": {
  "slug": "dumb-primer-on-recursive-zk-proofs",
  "content": "# A dumb primer on recursive ZK proofs \n\nHow do I understand recursive ZK proofs? / A better way.\n\nBy **Liam Zebedee ([@liamzebedee](https://twitter.com/liamzebedee))** and **XYZ**\n\n**Acknowledgements**. This came directly out of convos with Lord from Biblioteca, Francesco from Apibara, Guiltygyoza, in the entire StarkNet hacker house at ETHLisbon 2022. Thank you to them, and the organisers of that house.\n\nPS: Don't feel scared by the length - the sections are designed to be skippable.\n\n## Part 1: memoization.\n\nIn JavaScript, we have a common optimization pattern called [memoization](https://en.wikipedia.org/wiki/Memoization). For those not familiar, memoization is kind of like caching the results of calling a function, so when you call it again with the same inputs, you get the result from the cache.\n\nMemoization is usually implemented as a higher-order function, meaning it takes a function as input and returns another function. Here's an idiot's implementation of `memoize(x)` in Python:\n\n```py\ncache = {}\ndef memoize(f):\n    # The memoized version of f(x).\n    def f2(x):\n        if not x in cache:\n            cache[x] = f(x)\n        return cache[x]\n    return f2\n```\n\n**Learning**: memoization is wrapping a function with a results cache.\n\n## Part 2: ZK proofs - a practical walkthrough in Cairo.\n\nNow let's recap ZK proofs - a shorthand for ZK-STARK's and ZK-SNARK's. \n\nA ZK proof is a way to prove computation, in such a way that verifying that proof takes less time than running it. For a program that takes $N$ computational steps, we can prove it and verify that proof in $O(log^2 N)$ steps using STARK's - an exponential speedup. I've written a fairly good summary of this on [this Twitter thread](https://twitter.com/liamzebedee/status/1516241618919374851)). \n\nWhat does ZK proofing look like? Usually you write a \"provable\" program using a language like [Cairo](https://www.cairo-lang.org/), which compiles to bytecode, which is executed in a VM that generates a \"trace\" - basically, a dump of the values of the VM's CPU registers over time. This trace is then proven using a prover (see [Giza](https://github.com/maxgillett/giza), an open-source one I worked on), and out the other side, you get a binary blob. \n\nFor example:\n\n```cairo!\n# program.cairo\nfunc fib(n) {\n    if n == 0 {\n        return 0\n    }\n    if n == 1 {\n        return 1\n    }\n    return fib(n-1) + fib(n-2)\n}\n```\n\n\nCompile, run and generate the trace:\n\n```shell!\n# Compile.\ncairo-compile ./program.cairo --output ./program.json\n\n# Run.\ncairo-run --program=program.json --layout=all --memory_file=memory.bin --trace_file=trace.bin\n```\n\nNow prove the trace:\n\n```shell!\ngiza prove --trace=trace.bin --memory=memory.bin --program=program.json --output=proof.bin\n```\n\nThe proof is generated as a file, `proof.bin`. The proving step will take longer than running the program, but the verification will be much shorter. We can verify it like so:\n\n```shell!\ngiza verify --proof=proof.bin\n```\n\nNow, how do we know what we're actually verifying? Obviously it could be a proof of anything. In some ZK systems like [circom](https://github.com/iden3/circom), we have two separate compilation outputs - the proving circuit and the verification circuit, the latter of which we would use to authoritively verify a program run. In Starkware, they have a primitive higher up the stack called the \"bootloader\", which a **known program** that will execute any program given to it. \n\nHow do we know we're running the bootloader? The program hash is exposed as public memory input into the program run, and because it's public memory, we can access it as part of the proof in `proof.bin`. \n\nThis is what it looks like, conceptually:\n\n```cairo!\nfunc bootloader(code: felt*, program_hash: felt, input: felt*) {\n    # communicate with the verifier that we are executing this program.\n    assert hash(code) == program_hash\n    # write program_hash to public_memory\n    \n    # execute\n    push input to memory\n    jump code\n    # equivalent to f(x), where f=code, x=input\n}\n```\n\nTo illustrate how this works, we might have `bootloader.py` which will setup the bootloader to run our program. Then proving looks the same:\n\n```py!\n# This will write the program.json and the input data (fib(20)) to the bootloader's memory slot.\npython3 bootloader.py load --program program.json --entrypoint \"fib(20)\" --output-memory memory.bin\n\n# Now we run using the bootloader, which will run fib(20).\ncairo-run --program=bootloader.json --layout=all --memory_file=memory.bin --trace_file=trace.bin\n\n# Same steps for prove.\ngiza prove --trace=trace.bin --memory=memory.bin --program=program.json --output=proof.bin\n\n# Verify.\n# a. Verify we are running program.json, by checking the program_hash == H(program.json) inside proof.bin.\npython3 bootloader.py verify-invocation --proof proof.bin --program program.json --entrypoint \"fib(20)\"\n# b. Verify the proof.\ngiza verify --proof=proof.bin\n```\n\nAnd that's basically the full runthrough of a real ZK-STARK system, minus the bootloader.\n\n**Learning**: We write a program in Cairo, compile it to bytecode, run it to generate a trace, prove the trace, and then verify the proof. To verify what we're verifying, we use something called a bootloader, which communicates the program hash in the proof data via public memory.\n\n## Part 3: Recursive ZK proofs.\n\nSo what the fuck is recursive ZK proofing? \n\nIt's alien technology, that's what it is. We like communicating with aliens. But like most aliens, you can't understand what the fuck they're talking about at first.\n\nThe first lesson - recursion in proofing does not look like recursion **in any other language you're familiar with**. There is no \"call stack\" that is growing in size. There is no \"calling the same function\". So I think the analogy is completely stupid, tbh.\n\nRecursive proofing is this idea of \"verifying proofs within proofs\". Basically, you have `giza prove`, but this program is actually implemented in Cairo. So it's an API you can call within your ZK programs, which allows you to verify proofs from other program invocations. \n\nThis is fucking magic. Because you can verify a proof that verifies other proofs, which verifies other proofs...and eventually you get a single 20kB file which succinctly verifies an entire blockchain's history. See [ZeroSync for a genuine attempt to do this on Bitcoin](https://github.com/lucidLuckylee/zerosync). \n\n**But**, it's a terrible analogy. The recursion occurs conceptually, not inside of the programming language. It actually just looks like invoking a pure function. \n\nThe only language that implements recursive proofing right now is Mina. I've [written an example which uses recursion in ZK proofs](https://gist.github.com/liamzebedee/8d2efbb105d3b474dbe752d526cc9d27), and this is what it looks like using their API:\n\n```ts\n# From https://gist.github.com/liamzebedee/8d2efbb105d3b474dbe752d526cc9d27\n\n        transfer: {\n            privateInputs: [SelfProof],\n\n            method(t1: Transaction, t0: SelfProof<Transaction>) {\n                // Verify the state at t=0 was computed correctly.\n                t0.verify();\n\n                // State: utxo signature validation.\n                const t1_sighash = t1.sighash();\n                const msg = [t1_sighash];\n```\n\nt0 is a `SelfProof`, basically a class which is a proof. `verify()` is a method. Does that look like recursion to you? No. \n\n**Learning**: Recursive ZK proofs are extremely powerful. But the recursion occurs conceptually, not like it looks like in normal programming (no call stacks, no calling the same function).\n\n\n## Part 4: A better analogy for recursive proofs.\n\nSo here's the meat - I think memoization is a much better way to imagine recursion in ZK proofing. Coming back to the beginning of our article, this is `memoize(x)`:\n\n```py\ncache = {}\ndef memoize(f):\n    # The memoized version of f(x).\n    def f2(x):\n        if not x in cache:\n            cache[x] = f(x)\n        return cache[x]\n    return f2\n```\n\nThe funny thing is, a ZK proof is kind of like a secure cache. If I compute `fib(n=100)`, and give you a number, you can't trust me without running the `fib(n=100)` which is $O(N)$ steps. But what if I prove `fib(n=100)` and give you the proof? Then it costs you $O(log^2 N)$ steps to trust me, by verifying the proof. \n\nWhat might this look like applied to our memoize helper?\n\n```py\ncache = {}\ndef memoize(f):\n    # The \"securely\" memoized version of f(x).\n    def f2(x):\n        if not x in cache:\n            cache[x] = prove(f(x))\n        return verify(cache[x])\n    return f2\n```\n\n**Learning**: Another way to understand recursive proofs is through the idea of a \"secure cache\". The cache values are checked automatically using the `verify` function from Starkware.\n\n\n## Part 5: A better API for recursive proofs.\n\nRecursion is a fitting analogy conceptually, but when programming, we might think of a more ergonomic API design. What if we could annotate any function as `provable`? e.g.\n\n```cairo!\n# program2.cairo\n\n#[provable]\nfunc fib(n) {\n    if n == 0 {\n        return 0\n    }\n    if n == 1 {\n        return 1\n    }\n    return fib(n-1) + fib(n-2)\n}\n```\n\nAnd in other places, we could interact as so:\n\n```cairo!\n# program2.cairo\n\nfunc run_1() {\n    proof = fib.prove()\n}\n\nfunc run_2(proof: felt*) {    \n    // Verify the proof.\n    val = fib.verify(proof)\n}\n```\n\nBut why have `prove`/`verify` splattered throughout our code? Could we go more ergonomic? Why not have this built into the runner? Maybe we could have a shared `cache` for all function invocations, and the runtime would automatically memoize and check the cache. \n\nHere's how this might work:\n\n * a public area of memory dedicated to the cache.\n * cairo-run and giza are modified to accept \"secure cache entries\" - proofs.\n * the Cairo runtime automatically rewrites functions annotated as \"provable\" to check this cache for results, thus removing the need for the programmer to write \"verify\".\n\nThe potential workflow:\n\n```cairo!\n# program3.cairo\n\n#[provable]\nfunc fib(n) {\n    if n == 0 {\n        return 0\n    }\n    if n == 1 {\n        return 1\n    }\n    return fib(n-1) + fib(n-2)\n}\n\nfunc main1() {\n    val = fib(100)\n}\n\nfunc main2() {\n    val = fib(100)\n}\n```\n\nThere are no verify API calls here. The verification occurs automatically inside the Cairo runtime - `cairo-run`. We begin by running the program:\n\n```shell!\n# Compile.\ncairo-compile ./program3.cairo --output ./program3.json\n\n# Run main1.\ncairo-run --program=program3.json --entrypoint \"main1\" --layout=all --memory_file=memory.bin --trace_file=trace1.bin --cache_file=cache1.json\n\n# Prove main1.\ngiza prove --trace=trace1.bin --memory=memory.bin --program=program3.json --output=proof1.bin\n```\n\nThis might generate `cache1.json` looking like:\n\n```json\n[\n    {\n        \"name\": \"fib\",\n        \"code\": \"00a00e0fd0bca...\",\n        \"hash\": \"010e0f0dd0bca...\",\n        \"cache\": {\n            \"100\": \"PROOF_DATA\"\n        }\n    }\n]\n```\n\nThe second invocation, `main2`, we run the same function but we supply the cache of function invocations. \n\n```shell!\n# Run main1.\ncairo-run --program=program3.json --entrypoint \"main2\" --layout=all --memory_file=memory.bin --trace_file=trace2.bin --cache_file=cache1.json\n\n# Prove main1.\ngiza prove --trace=trace2.bin --memory=memory.bin --program=program3.json --output=proof1.bin\n```\n\nAnd there you have it - an API for recursive proofs that doesn't litter our code. Comparing this against the Mina code, where we have to be **manually passing around proofs** and intermixing our code with these verify functions, this is much more ergonomic. `SelfProof<Transaction>.verify()` vs. `#[provable]`. It feels very clean.\n"
 },
 "how-i-found-my-new-house-thru-6-degrees": {
  "slug": "how-i-found-my-new-house-thru-6-degrees",
  "content": "# How I found my house through 6 degrees of separation.\n\n*21 Dec 2022*.\n\nA very interesting thing happened to me recently. When I got back from overseas, I had to find a house at very short notice. With 5 days to go, I had no idea what to do - but this wasn’t my first time. :P In my moments of pure chaos, I posted to my instagram story - “hey does anyone have a place in Sydney”.\n\nOut of nowhere, a mate I’d met at a party some 6mo earlier responded. “Yeah man, weirdly we just had something open up”. I went there and boom, I was on my feet thanks to Pete. What a legend. With 3 days to go, I had found the most fucking amazing house with the most amazing people. How in hell did this happen?\n\nWell, I knew Pete because of this party. We both started pretending to be Swedish people and started talking about having this shared grandmother called “Olaf Olaffson”. People kept coming up to us and it’s like they were unknowingly invited into our own little world. It was hilarious.\n\nThe party was actually at the house of a Swiss girl I’d met the week earlier, Camilla. We were both invited to this boat party because of Jordan, who’d met Camilla’s sister while he was in Europe. It was Jordan’s birthday.\n\nIt’s funny because Jordan lives in Sydney, is from Brissy (like me), but I actually met the first time in Sweden, some 2yrs earlier! What were we doing in Sweden? My mate Felix has this annual waffelfesten, where they just cook a bunch of waffles and have a party.\n\nFinally, how did I meet Felix? Well, when I moved from Brisbane to Sydney, I studied IT. And one day I walked into the first “computer networking” class, looked around, and tried to find the guy who looked least like an incel. That’s how I met Felix. This was about 5yrs ago now, when he was on exchange in Sydney from Sweden.\n\nThis all was made really interesting when the day I moved in, Felix had literally just come back to Sydney to visit. Let’s draw the chain of people:\n\nFelix (2015, Sydney) -> Jordan (2019, Stockholm) -> Camilla (2022 Mar, Sydney) -> Petey (2022 Dec, Sydney) -> my current place\n\nHow interesting and serendipitous is this? Over the span of 7 years, I’d lived in Brisbane, Sydney, then Amsterdam, and then Sydney again. At each point I met one of these people. Because of Felix, I guess I found a place to live?? And the day I moved in, person #1 met person #4.\n\nThere is something very weird I realised when this happened. If you look at the dots connecting backwards, you can start to imagine what the world really looks like. You know that there are probably things out there that you don’t know about.\n\nI didn’t know there was a 1970’s Italian Art Deco mansion in Randwick. Somehow, in this fucking insane universe of possibilities, I went from having no place to go with 5 days notice, to living in literally the best place I’ve ever lived. But maybe it’s not so insane?\n\nMaybe a more realistic way to look at the world is that it’s really close. After all, all of these people interacted at a distance of 10’s of thousands of kilometres - Felix is from Stockholm, Sweden, Jordan is from Brisbane, Australia, Camilla and her sister who Jordan met are from Switzerland, Peter is in Sydney. Even Sydney to Brisbane is at least 1000km away.\n\nMaybe it’s easier to understand visually. This is the path to how I found my house.\n\n![img](https://i.imgur.com/NIw3S8t.png)\n\nWhat happens if we think about it in reverse?\n\n![img](https://i.imgur.com/zBgBC1O.png)\n\nWho do we have to meet to find [x]? Who do we have to meet to find the one? Or find that new job we’ve been searching for? Or whatever?\n\nOnce you think about it this way, you realise that a lot of things in life are dependent on the path you take. And when you start looking at the world through path dependence, things make a lot more sense.\n\nThere are a lot of variations of this idea of looking at the world. You might have heard quotes like “*being in the right place at the right time*”. Or “*luck is opportunity plus preparation*”.\n\nOne way to reframe these quotes is this idea that the right place is actually where you are in your path. For example - I was in the right place to find this house. The preparation was me being vocacious, ADHD, extroverted, whatever. The luck / right-time was that Pete’s housemate had literally just left, and they were searching for someone. Without having met all of these wonderful people, it wouldn’t have happened.\n\nThere is actually a lot of examples of this in different areas of life (see the appendix for some detail). Business, relationships and love, the etymology and evolution of languages, all of these things evolve via a path you cannot predict. But you can understand the mechanism as to how.\n\nI guess my conclusion is now I see things differently. Now I look at them like this:\n\n![img](https://i.imgur.com/CxYnUNJ.png)\n\nYou don’t know when the paths will connect up. But at the same time, you do know that they connect. The question is, how do you find out where to walk?\n\n*“You can’t connect the dots looking forward; you can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future. You have to trust in something – your gut, destiny, life, karma, whatever. This approach has never let me down, and it has made all the difference in my life.”*\n\nIt’s funny to [quote Steve Jobs](https://www.youtube.com/watch?v=UF8uR6Z6KLc) but he’s got a point. Whatever the way you look at it, you have to begin with trust. Trust that you can figure it out, trust your instinct that you know where things will lead to. It will make all the difference.\n\nSo maybe let's evaluate things not on just what they are, but where they might lead us to. Each project is a stepping stone to other projects, each person a connection to a wider world. Life is full of colour, you just have to look for it. \n\n## Appendix\n\n### Examples of path dependency.\n\n**Social networks**. Why did Facebook beat Myspace, despite having the same features? Their CTO wrote about [“The Path Matters”](https://boz.com/articles/the-path-matters).\n\n“At each point in the punctuated growth of our user base we have had to refine our product to make it more general and more universal. We react to their feedback and usage of the product. At the same time, our user base is itself evolving not only in composition but also in their comfort level with new technologies”.\n\n**Business**. “Whenever you want to understand someone’s success, understand how they solved for distribution”. Lex Friedman is a deeply popular podcaster now, but did you know how he got there? I would suggest reading [@visakanv’s thread](https://twitter.com/visakanv/status/1576888603338080258).\n\n**Relationships and love**. When you are dating early in your 20’s, there are a lot of single people. As you get older, there are less. These things vary from country-to-country - I remember one Italian telling me most of them don’t really finish uni and enter the workforce until they’re 28. That’s gotta be different to Australia, where people begin to settle around 24. Do you move to Italy then? That’s a choice. Your choices early on - whether to invest more and settle or search more and explore - these are paths. And even the idea of “what the one looks like” is a path in itself as you try to figure out what you want.\n\n**The English language.** You can see the evolution of languages as a pattern of path dependency. English is one of the weirdest and most highly irregular languages for this reason - it began as Old English, a dialect of German, but after the invasions of Scandies and the French, it started evolving into a weird mix of Germanic, Latin, and French influences. Take this from “[Why is English so weirdly different from other languages](https://aeon.co/essays/why-is-english-so-weirdly-different-from-other-languages?https://aeon.co/essays/why-is-english-so-weirdly-different-from-other-languages?)”:\n\n> *“The die was cast: English had thousands of new words competing with native English words for the same things. One result was triplets allowing us to express ideas with varying degrees of formality. Help is English, aid is French, assist is Latin. Or, kingly is English, royal is French, regal is Latin – note how one imagines posture improving with each level: kingly sounds almost mocking, regal is straight-backed like a throne, royal is somewhere in the middle, a worthy but fallible monarch.”*\n\nEtymology is a fascinating look into how path dependence works, and very accessible. [Every name has a history](https://liamz.co/blog/the-roots-of-mountain-and-other-language-motifs/) and you can discover it by looking it up on [wikitionary](https://en.wiktionary.org/wiki/Wiktionary:Main_Page). For example, I did this for my own name. “Liam” derives from Willhelm, meaning protector. “Willhelm” originates from a German phrase, meaning will of the helmet. For more interesting words, @visakanv has an amazing [twitter thread here](https://twitter.com/visakanv/status/1010793300414959616). And another one on the [etymology of programming](https://twitter.com/hillelogram/status/1357492666573979649) things cool too.\n\n**“Yield farming” aka how meme tokens get their value.** Although he’s disgrace, SBF articulated a deeply interesting thing about [“yield farming” in an interview](https://www.bloomberg.com/news/articles/2022-04-25/sam-bankman-fried-described-yield-farming-and-left-matt-levine-stunned) with Matt Levine. Yield farming is a process of reification, that is entirely different to the classical Ponzi structure. While you may disagree on what’s valuable (which is fair), it’s interesting to learn about why people value these things and how that even comes into action - the path.\n\n### A more fleshed out diagram of the path I took to getting this house.\n\n![img](https://i.imgur.com/OODAFFh.png)![img](https://i.imgur.com/T5Ih8f5.png)"
 },
 "loot-fog-of-war-private-information-zk": {
  "slug": "loot-fog-of-war-private-information-zk",
  "content": "# Loot Fog of War, private-information games, and ZK STARK's.\n\nBy **Liam Zebedee ([@liamzebedee](https://twitter.com/liamzebedee) aka SugarLord)**.\n\n**Context**: Loot Assassins and their [fog-of-war problem](https://twitter.com/lordOfAFew/status/1588204458420338689)\n\n# Intro.\n\nIf you've ever played Age of Empires, Guild Wars, or any other real-time strategy (RTS) game, you know it's a fun time. Gathering resources, assembling armies and then plotting attacks against other players is awesome - and it's especially fun given a multiplayer environment, what sorts of strategies emerge. In recent years, money has become more a part of online gaming than ever before - for example, [EVE Online's $1M+ battle](https://www.vice.com/en/article/9kn745/eve-online-million-dollar-battle). \n\nThe natural question to ask is - why not on-chain? Blockchains are in essence, an open game platform, where anyone can permissionlessly build new game items and lore. Imagine a version of World of Warcraft, where anyone could create their own quests, with their own items, game mechanics, and so on. This is being made today in a project/network called [Loot / the Lootverse](https://www.lootproject.com/). What's more - blockchains connect you with a wide range of infrastructure - simply implementing a game that adheres to the ERC20 standard for currency, or the ERC721 standard for items, means your game's items and currency are automatically displayed in a wider web of apps - users can show off their character's items in social networks like [Farcaster](https://www.farcaster.xyz/), they can trade items on [Uniswap](uniswap.org/) or [OpenSea](https://opensea.io/), and much more. \n\nThe challenges with building rich on-chain games really fall into a couple of categories:\n\n 1. Scaling computation.\n 2. Scaling state.\n 3. Private information.\n\nComputation/state are notoriously expensive on Ethereum. You can't run a physics engine on Ethereum, for example. However, there's a new technology which is making this *exponentially* cheaper - STARK's. STARK's are a type of cryptography that you can use to prove computation - something very useful in a trustless environment like blockchain. Using STARK's, we can verify computation in $O(log^2 N)$ through checking a proof, as opposed to verifying it naively by re-rerunning it, which is $O(N)$. So computation is getting a *lot* cheaper. \n\nWhat about state? Our systems are getting ripped apart and specialised by the day. The insanely high cost of storing data (state) is mainly due to the lack of horizontal scalability in blockchains - since every node must transmit and store every transaction, it's very expensive per byte. Again, STARK's have changed the game here - using something called recursive ZK proofs, it's possible to aggregate the proven state roots of multiple blockchains in parallel - without needing to transfer the raw state itself. This essentially will unlock horizontal scalability, something we've had in web2 databases for a long time. The general term people are using here is L3's, but honestly I think it's a dumb term. (side-note: I'm building a horizontally-scalable decentralized database based this called [Goliath](https://glissco.notion.site/Goliath-whitepaper-7cf50163301a42c7b264b02f248a6a07), check it out)\n\nThe last problem is \"private information\". And this is the most interesting and why I'm writing this post.\n\n### Private information in games.\n\nWhen you're playing Age of Empires, there's this concept of \"fog of war\" - the part of the map you cannot see. It's *essential* to the game - you have no idea what the other players are doing, what their army looks like, etc. This is _private information_. \n\nWhen you put a game (or financial protocol) on-chain, you are essentially publishing a bug bounty. The blockchain is an extremely adversarial environment, where every strategy is being explored by a decentralized community of agents - simple trades are [frontrun and \"sandwiched\"](https://coinmarketcap.com/alexandria/article/what-are-sandwich-attacks-in-defi-and-how-can-you-avoid-them), agents run specialised software to simulate pending transactions and re-submit them as their own if they can claim the profit, and then there is *another* layer of metagame where there are other players who engineer [contracts to purposefully honeypot](https://www.mev.wiki/attempts-to-trick-the-bots/salmonella) those who simulate and hijack other tx's. Point is - if there is a vector, it will be exploited.\n\nAll public information, can, and will, be exploited. But we *need* our Age of Empires clone to have private information - if any player (human or some weird deep reinforcement learning AI) could see where the other players army is, they could win easily. Would that EVE Online battle still work if all the info was public? No.  \n\n### The \"fog of war\" problem.\n\nSo how could we implement it? First, let's work with a simple definition of the problem-\n\n**Problem**. There are two players, each building armies. We want to build an on-chain game where the players can move their armies around a map. _If the armies meet_, meaning their coordinates match up, a battle is started and their positions are revealed. Until that point, all information about the location of armies is private from other players. \n\nYou can imagine the state looks something like this:\n\n```python!\nclass Game:\n    player1: Player\n    player2: Player\n\nclass Player:\n    army_pos: [x, y]\n```\n\nThe actual game logic is on-chain, and might look like this: \n\n```cairo!\n# army.cairo\nfunc move_army(x: felt, y: felt) {\n    players[msg.sender].army_pos = [x,y];\n    \n    if(players[0].army_pos == players[1].army_pos) {\n        commence_battle();\n    }\n}\n```\n\nNow obviously, the player state in this contract is public. How do we make the state private? \n\n### Basic solutions - encryption, hashing.\n\nThe first intuition might be to encrypt it, but remember - the logic can't be verified on-chain unless it is publicly decrypted - which would make the information public, thus negating our objective. There are methods of doing computation on encrypted data called [homomorphic encryption](https://en.wikipedia.org/wiki/Homomorphic_encryption), but they aren't ready yet.\n\nThe second intuition might be to hash it - thus obscuring the information from public view. \n\n```cairo!\n# army.cairo\n\n# position_hash is set as hash(x ++ y).\nfunc move_army(position_hash: felt) {\n    players[msg.sender].army_pos = position_hash;\n    \n    if(players[0].army_pos == players[1].army_pos) {\n        commence_battle();\n    }\n}\n```\n\nThis _could work_! But it's essentially [security-by-obscurity](https://en.wikipedia.org/wiki/Security_through_obscurity) - anyone could go to the effort of precomputing all of the hashes for positions on the map, and basically reverse-engineer a player's position (this is called a [rainbow table](https://en.wikipedia.org/wiki/Rainbow_table)). \n\nWhat if we salted the hash? So each hash is completely unique. This is interesting.\n\n```cairo!\n# army.cairo\n# position_hash is set as hash(x ++ y ++ salt), where salt=random().\nfunc move_army(position_hash: felt) {\n    players[msg.sender].army_pos = position_hash;\n    \n    if(players[0].army_pos == players[1].army_pos) {\n        commence_battle();\n    }\n}\n```\n\nBut then the position collision logic would never work. Hmmmm. \n\n### A trusted dungeon-master to custody private info?\n\nWhat if there was another party that we trusted to store our private information (positions)? And they performed the army collision logic? This would make the game centralized, but let's play with the idea. Let's call them the **dungeon master**. How would that look? \n\n```python!\n# dungeon_master.py\n\n# On-chain.\nclass ArmyContract:\n    def commence_battle(player1, player2):\n        # calls on-chain contract.\n\n# Off-chain.\nclass DungeonMaster:\n    players = []\n        \n    def move_army(x: felt, y: felt):\n        players[msg.sender].army_pos = [x,y]\n    \n        if(players[0].army_pos == players[1].army_pos):\n            ArmyContract.commence_battle()\n```\n\nThe `DungeonMaster` stores the player's locations, and runs the collision detection itself. When it detects two armies have met, it commences a battle by calling the on-chain contract. \n\nThe problem here is that (1) the logic is not verifiable and (2) the state is not verifiable. \n\n### A trustless dungeon-master.\n\nWhat if we built something like a state channel? Basically, players sign transactions and submit them off-chain to the DungeonMaster. The DungeonMaster stores the latest state, and when there is a collision, the it submits those transactions on-chain, updating their positions to the latest state, and then commences the battle. This is in essence, an [optimistic rollup](https://ethereum.org/en/developers/docs/scaling/optimistic-rollups/) with a centralized operator that can custody private state. \n\nThat's cool! But the problem is it's complex. Is there a better way? \n\n#### Fraud proofs and Validity proofs.\n\nOptimistic rollups are based on optimism - you optimistically assume that the state which is posted on-chain was computed correctly, and you hope that someone is verifying it was. If there is fraud, then the complete transaction is re-run on-chain in $O(N)$ time (in practice, it's a bit more efficient due to [interactive fraud proofing](https://medium.com/offchainlabs/interactive-fraud-proofs-arbitrums-secret-sauce-debc3b019418) but it's essentially the same worst-case).\n\nA better approach is validity proofs, which are based in ZK. A ZK proof is by nature, nihilistic - it doesn't give a shit whether you believe in maths or not, but if you do (which we do) - you can prove computation in $O(log^2 N)$ efficiency. This is much better. \n\n### A trustless VM dungeonmaster.\n\nSo imagine this: the DungeonMaster operates as a Cairo VM. It processes each transaction. When there is a collision, it generates a proof of processing all of these transactions, and submits it to the remote blockchain - StarkNet. \n\nWhat does this look like? We return to our original Cairo model:\n\n```cairo!\n# army.cairo\nfunc move_army(x: felt, y: felt) {\n    players[msg.sender].army_pos = [x,y];\n    \n    if(players[0].army_pos == players[1].army_pos) {\n        commence_battle();\n    }\n}\n```\n\n#### On the DungeonMaster chain.\n\nLet's imagine the \"position\" state as its own little state machine, where players invoke the `next_position` function to move, and our proof basically returns the latest position:\n\n```cairo!\n# dungeon-master/army.cairo\nfunc next_position(x: felt, y: felt) {\n    players[msg.sender].army_pos = [x,y];\n    \n    if(players[0].army_pos == players[1].army_pos) {\n        commence_battle_hook();\n    }\n}\n```\n\nThis contract is deployed in a local Cairo VM. Every transaction submitted will change the state of this VM. When a collision is detected, we need to do two things: \n\n 1. generate a proof of the latest state and\n 2. submit this proof to the remote blockchain. \n\nWe need some way to interrupt and hook this event. For now, we're just going to assume we've got this figured out (maybe using events), and call it `commence_battle_hook`. \n\n`commence_battle_hook` will save the current state of the VM (trace), prove it using a prover like [Giza](https://github.com/maxgillett/giza), and then send that proof to the remote StarkNet blockchain to commence the battle. (see my article on [recursive proofing to see how proving might look](https://hackmd.io/@liamzebedee/BydanTDSi)).\n\n#### On the StarkNet chain.\n\nOn the StarkNet chain, we will process this proof, containing the latest state - the positions of the two players - and then commence the battle. \n\n```cairo!\n# starknet/army.cairo\n\n# Applies the latest \"position\" state from the Dungeon Master, shown by the proof.\nfunc process_moves(move_proof: felt*) {\n    (valid, (x, y)) = next_position.verify(move_proof);\n    assert valid;\n    players[msg.sender].army_pos = [x,y];\n}\n\nfunc commence_battle(player1_position_proof: felt*, player2_position_proof: felt*) {\n    process_moves(player1_position_proof);\n    process_moves(player2_position_proof);\n    \n    if(players[0].army_pos == players[1].army_pos) {\n        commence_battle();\n    }\n}\n```\n\nAnd there we have it! The dungeon master is in essence its own little blockchain, with private state that it custodies. Players interact with the DM, and when there is a collision, it proves it, and sends it to a remote blockchain (StarkNet), which will commence the battle. The dungeon master cannot perform malicious behaviour, because it runs a verifiable state machine, whose transitions are proven using STARK's.\n\nThis example glosses over a couple of security concerns for simplicity's sake, though they're all quite addressable: \n\n * **How do we authenticate tx's?** The user signs txs, just like in a rollup. The signature validation would occur inside `next_position`.\n * **How do we ensure fair sequencing and fair ordering?** Right now, these off-chain transactions can be re-ordered or even dropped entirely. Like a regular blockchain, there needs to be a sequencer - arguably this could just be the DM. This sequencer would sign every tx it processes, and probably have a stake that could be slashed if it is shown that it has behaved inconsistently (signing more than 1 different sequence of txs).\n\n\n## Conclusion.\n\nPrivate information in video games is pretty damn essential. And on-chain games are going to be awesome, given we can mess around with real money and DAO's. How do we get private info though? We can tradeoff a little bit of trust by using a trusted 3rd party, the dungeon master, to custody our private information. Unlike a centralized party that might just sign state updates (like a ChainLink oracle), we're only trusting this third party to keep our information private - the rest of their job (computation) is secured by STARK validity proofs. \n\nThis whole example is really just a stepping stone on the way to what people keep calling \"L3 scaling\". I wrote it up pretty quickly to illustrate the direction this is all heading in - towards an even more generalised cross-chain state transfer using ZK proofs. \n\n### Future research.\n\n### A generalised state machine.\n\nCan we go harder? What if we didn't have to write a `process_moves` function? What would something more generalised look like? \n\n_This is left as an exercise to the guilty gyoza._\n\n<!-- But it gets *even better*. \n\nZK proofs can prove other ZK proofs. To illustrate, let's return to our last idea:\n\n> the DungeonMaster stores the latest state, and when there is a collision, it submits those transactions on-chain, updating their positions to the latest state, and then commences the battle.\n\nInstead of processing $N$ txs on-chain, a ZK proof will be $log (N)$. It looks something like this:\n -->\n \n \n### MPC, Homomorphic computation\n\nHonestly private info and multi-party computation / FHE (fully homomorphic encryption) could very much solve this, though we don't know how yet. \n\nThere are a couple groups specialised in this area that'd have some good ideas on this:\n\n - [cronokirby](https://twitter.com/messages/1712298445-1102909424912478208)\n - [henry](https://twitter.com/hdevalence) and the [penumbra](https://twitter.com/penumbrazone) team\n\n\n\n\n"
 },
 "second-order-effects-of-ai-and-crypto-2022": {
  "slug": "second-order-effects-of-ai-and-crypto-2022",
  "content": "\n - as generative AI gets better, cost of producing content goes to 0\n - yet attention is still scarce\n - so why not build a marketplace for attention? @glissco\n - curation is extremely valuable (@ennntropy)\n - everything has nonzero price, even shitcoins like ETHPOW\n \n   this is the “aha moment” we had with mergeswap cc @0xmarcello\n\n  - it only takes one person to put your NFT on opensea, and then it has a “price”. Magic experience that everyone should try\n\n"
 },
 "take": {
  "slug": "take",
  "content": "What is the idea of Take?\n\nWhat we recognise, is that memes have power.\n\n"
 },
 "test": {
  "slug": "test",
  "content": ""
 }
};

export default data